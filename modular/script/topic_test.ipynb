{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "decoding to str: need a bytes-like object, int found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/mayag88/Documents/Code/Bachelor_FINAL/modular/script/topic_test.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mayag88/Documents/Code/Bachelor_FINAL/modular/script/topic_test.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m num_clients \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mayag88/Documents/Code/Bachelor_FINAL/modular/script/topic_test.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Split features into two groups using LDA\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mayag88/Documents/Code/Bachelor_FINAL/modular/script/topic_test.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m dictionary \u001b[39m=\u001b[39m corpora\u001b[39m.\u001b[39;49mDictionary([\u001b[39mrange\u001b[39;49m(features\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m])])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mayag88/Documents/Code/Bachelor_FINAL/modular/script/topic_test.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m lda_model \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39mLdaModel(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mayag88/Documents/Code/Bachelor_FINAL/modular/script/topic_test.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     corpus\u001b[39m=\u001b[39m[\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mayag88/Documents/Code/Bachelor_FINAL/modular/script/topic_test.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         [(word_id, \u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m word_id \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(features\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mayag88/Documents/Code/Bachelor_FINAL/modular/script/topic_test.ipynb#W1sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mayag88/Documents/Code/Bachelor_FINAL/modular/script/topic_test.ipynb#W1sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mayag88/Documents/Code/Bachelor_FINAL/modular/script/topic_test.ipynb#W1sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Generate non-IID data for each client\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/gensim/corpora/dictionary.py:78\u001b[0m, in \u001b[0;36mDictionary.__init__\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_nnz \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m documents \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_documents(documents, prune_at\u001b[39m=\u001b[39;49mprune_at)\n\u001b[1;32m     79\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_lifecycle_event(\n\u001b[1;32m     80\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcreated\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     81\u001b[0m         msg\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbuilt \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m from \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_docs\u001b[39m}\u001b[39;00m\u001b[39m documents (total \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_pos\u001b[39m}\u001b[39;00m\u001b[39m corpus positions)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     82\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/gensim/corpora/dictionary.py:204\u001b[0m, in \u001b[0;36mDictionary.add_documents\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m    201\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39madding document #\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, docno, \u001b[39mself\u001b[39m)\n\u001b[1;32m    203\u001b[0m     \u001b[39m# update Dictionary with the document\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdoc2bow(document, allow_update\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)  \u001b[39m# ignore the result, here we only care about updating token ids\u001b[39;00m\n\u001b[1;32m    206\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mbuilt \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m from \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m documents (total \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m corpus positions)\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_docs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_pos)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/gensim/corpora/dictionary.py:246\u001b[0m, in \u001b[0;36mDictionary.doc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    244\u001b[0m counter \u001b[39m=\u001b[39m defaultdict(\u001b[39mint\u001b[39m)\n\u001b[1;32m    245\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m document:\n\u001b[0;32m--> 246\u001b[0m     counter[w \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(w, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mstr\u001b[39;49m(w, \u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m)] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    248\u001b[0m token2id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken2id\n\u001b[1;32m    249\u001b[0m \u001b[39mif\u001b[39;00m allow_update \u001b[39mor\u001b[39;00m return_missing:\n",
      "\u001b[0;31mTypeError\u001b[0m: decoding to str: need a bytes-like object, int found"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Load breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "features = data.data\n",
    "labels = data.target\n",
    "\n",
    "# Min-Max scaling on features\n",
    "scaler = MinMaxScaler()\n",
    "normalized_features = scaler.fit_transform(features)\n",
    "\n",
    "# Number of clients\n",
    "num_clients = 2\n",
    "\n",
    "# Prepare data for Gensim LDA\n",
    "corpus = [list(enumerate(doc)) for doc in normalized_features]\n",
    "\n",
    "# Split features into two groups using LDA\n",
    "dictionary = corpora.Dictionary([range(features.shape[1])])\n",
    "lda_model = models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=features.shape[1],\n",
    "    alpha=0,  # Set alpha to 0 for non-IID distribution\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Generate non-IID data for each client\n",
    "non_iid_data = []\n",
    "for client_id in range(num_clients):\n",
    "    topic_distribution = np.random.dirichlet(\n",
    "        np.ones(features.shape[1]) * 10, size=1\n",
    "    ).flatten()\n",
    "    document = lda_model.make_doc(\n",
    "        [\n",
    "            (i, topic_distribution[i] * features.shape[0])\n",
    "            for i in range(features.shape[1])\n",
    "        ]\n",
    "    )\n",
    "    non_iid_data.append(document)\n",
    "\n",
    "# Display the non-IID data\n",
    "for client_id, document in enumerate(non_iid_data):\n",
    "    print(f\"Client {client_id + 1} data:\")\n",
    "    print(\n",
    "        lda_model.print_topic(\n",
    "            lda_model.get_document_topics(\n",
    "                document, minimum_probability=0.0, per_word_topics=False\n",
    "            ),\n",
    "            num_words=features.shape[1],\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Now 'non_iid_data' contains non-IID data for each client based on LDA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
